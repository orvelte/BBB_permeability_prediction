# -*- coding: utf-8 -*-
"""BBB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_GQPuZw-g0EQ_iQDrTdlFZJ8BBIE_7OS
"""

# Note: Install these packages using pip before running the script:
# pip install rdkit-pypi
# pip install matplotlib==3.5.3
# pip install pandas numpy seaborn scikit-learn

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve
import joblib
import os

from rdkit import Chem
from rdkit import DataStructs
from rdkit.Chem import AllChem, DataStructs, Descriptors, Draw
from rdkit.Chem.Draw import SimilarityMaps
from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator
from rdkit.ML.Descriptors import MoleculeDescriptors

# Basic molecular analysis
m = Chem.MolFromSmiles('CC(CC1=CC2=C(C=C1)OCO2)NC ')

img = Draw.MolToImage(m)
# img  # Uncomment to display image in Jupyter

"""Is it the right molecule?

The beauty of RDKIT is it can quickly perform calculations about your molecule(s). Calculations include count the number of atoms in a molecule, count the molecular weight, etc. You could of course do this manually, but as you will see later, this can be exhausting for large datasets. Don't worry if you are not an expert of drugs. RDKIT is reliable enough.
"""

print("Number of atoms (without H):", m.GetNumAtoms())
#gets the number of non-carbon atoms

m2 = Chem.AddHs(m)
print("Number of atoms (with H):", m2.GetNumAtoms())
#includes the number of all the atoms

#calculate the entire molecular weight
print("Molecular weight:", Descriptors.MolWt(m))

"""Note descriptors are what the cheminformatics community calls features. As the name suggest, descriptors are descriptions of molecules. There are hundreds of these available. We will be working with 2D descriptors but there are also 3D descriptors - descriptors that provide information about a molecule's 3D structure.
Another useful feature from RDKIT is its ability to compare the similarities between molecules.
"""

mol1 = Chem.MolFromSmiles('CC(=O)NC1=CC=C(C=C1)O') # Paracetamol
mol2 = Chem.MolFromSmiles('CN1C=NC2=C1C(=O)N(C(=O)N2C)C') # Caffeine
mol3 = Chem.MolFromSmiles('CN1C2=C(C(=O)N(C1=O)C)NC=N2') # Theophylline
mol4 = Chem.MolFromSmiles('CC(CC1=CC2=C(C=C1)OCO2)NC') #3,4-Methylenedioxymethamphetamine

fp1 = AllChem.GetMorganFingerprint(mol1, 4)
fp2 = AllChem.GetMorganFingerprint(mol2, 4)
fp3 = AllChem.GetMorganFingerprint(mol3, 4)
fp4 = AllChem.GetMorganFingerprint(mol4, 4)

"""AllChem.GetMorganFingerprint(): This is a function from the RDKit library that calculates a specific type of fingerprint called a Morgan fingerprint. Morgan fingerprints are a common type of circular fingerprint used in cheminformatics."""

print("Similarity between Paracetamol and Caffeine:", DataStructs.TanimotoSimilarity(fp1, fp2))
print("Similarity between Caffeine and Theophylline:", DataStructs.TanimotoSimilarity(fp2, fp3))
print("Similarity between Theophylline and Paracetamol:", DataStructs.TanimotoSimilarity(fp3, fp1))
print("Similarity between MDMA and Paracetamol:", DataStructs.TanimotoSimilarity(fp4, fp1))
print("Similarity between MDMA and Caffeine:", DataStructs.TanimotoSimilarity(fp4, fp2))
print("Similarity between MDMA and Theophylline:", DataStructs.TanimotoSimilarity(fp4, fp3))

# Similarity maps
mol = mol2 # mol2 is already a Mol object
AllChem.ComputeGasteigerCharges(mol)
contribs = [mol.GetAtomWithIdx(i).GetDoubleProp('_GasteigerCharge') for i in range(mol.GetNumAtoms())]
fig = SimilarityMaps.GetSimilarityMapFromWeights(mol, contribs, colorMap='jet', contourLines=10)

"""Lab Task 2

For the second task, we will leverage descriptors to help us build effective ML models to predict drug biological activity. One such activity is whether a drug can cross the blood brain barrier (BBB). The BBB is a barrier that aims to prevent toxic molecules from reaching and consequently affecting our brain. It's a robust barrier and ensures our safety. However, it also prevents drugs from reaching the brain, which is a burden if you want to treat a brain-related disorder. So let's see if ML can be used to help scientists know if their drug can cross the BBB.

Your task will be to convert the SMILES notation from the dataset into 200 molecular descriptors for each drug in the dataset. Once featurised, use the molecular descriptors as inputs to predict whether a drug will cross the BBB. It should be routine by now to make sure EDA is first performed before jumping straight into ML training. If you decide to use a tree-based learner, don't forget to output the feature importance to see which molecular descriptors are key to the learner's performance.
"""

# Load the dataset
try:
    data_drug = pd.read_csv('../data/BBB_datasets.csv', encoding='latin-1')
    print("Dataset loaded successfully")
    print("Dataset shape:", data_drug.shape)
    print("Columns:", data_drug.columns.tolist())
except FileNotFoundError:
    print("Error: BBB_datasets.csv file not found. Please ensure the file is in the data/ directory.")
    exit(1)

# Select the SMILES column
smiles = data_drug['SMILES']

# Create a list for molecular descriptors
mol_descriptors = []

# Loop through every drug and calculate the molecular descriptors
print("Calculating molecular descriptors...")
for i, smile in enumerate(data_drug['SMILES']):
    moler = Chem.MolFromSmiles(smile)
    if moler is not None:
        try:
            calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0] for x in Descriptors._descList])
            vector = calc.CalcDescriptors(moler)
            mol_descriptors.append(vector)
        except Exception as e:
            print(f"Error processing SMILES {i}: {smile}, Error: {e}")
    else:
        print(f"Invalid SMILES at index {i}: {smile}")

print(f"Successfully processed {len(mol_descriptors)} molecules")

# Create an array of the molecular descriptors
cols_mols = [x[0] for x in Descriptors._descList]

desc_df = pd.DataFrame(mol_descriptors, columns=cols_mols)

print("Descriptor DataFrame shape:", desc_df.shape)
print("First few rows:")
print(desc_df.head())

# EDA
print("\n=== Exploratory Data Analysis ===")
print("Basic statistics:")
print(desc_df.describe())

print("\nMissing values:")
missing_values = desc_df.isna().sum()
print(missing_values[missing_values > 0])

# Remove rows with missing values
desc_df.dropna(inplace=True)
desc_df = desc_df.reset_index(drop=True)

print(f"\nDataFrame info after cleaning:")
print(desc_df.info())

# PCA
print("\n=== Principal Component Analysis ===")

# Standardize the data
scaler = StandardScaler()
desc_df_scaled = scaler.fit_transform(desc_df)

# Perform PCA
pca = PCA(n_components=2)
desc_df_pca = pca.fit_transform(desc_df_scaled)

print(f"Explained variance ratio: {pca.explained_variance_ratio_}")

# Get the corresponding class labels for the cleaned data
class_values = data_drug['Class'].iloc[desc_df.index]

# Map class values to colors
color_map = {'BBB+': 'red', 'BBB-': 'blue'}
colors = [color_map.get(cls, 'gray') for cls in class_values]

# Create scatter plot with color-coding
plt.figure(figsize=(10, 6))
scatter = plt.scatter(desc_df_pca[:, 0], desc_df_pca[:, 1], c=colors, alpha=0.7)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Drug Descriptors')

# Create legend
from matplotlib.patches import Patch
legend_elements = [Patch(facecolor=color, label=label) for label, color in color_map.items()]
plt.legend(handles=legend_elements, title='BBB Permeability')

plt.tight_layout()
plt.show()

# Machine Learning Implementation
print("\n=== Machine Learning Model Training ===")

# Prepare features and labels
X = desc_df_scaled  # Already standardized features
y = class_values    # BBB permeability labels

# Encode labels to numerical values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

print(f"Feature matrix shape: {X.shape}")
print(f"Label distribution: {np.bincount(y_encoded)}")
print(f"Classes: {label_encoder.classes_}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

print(f"Training set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")

# Define models to train
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(probability=True, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)
}

# Train and evaluate models
results = {}
trained_models = {}

for name, model in models.items():
    print(f"\n--- Training {name} ---")
    
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    
    # Store results
    results[name] = {
        'accuracy': accuracy,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }
    trained_models[name] = model
    
    print(f"Accuracy: {accuracy:.3f}")
    print(f"CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
    
    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Feature Importance Analysis (for Random Forest)
print("\n=== Feature Importance Analysis ===")
rf_model = trained_models['Random Forest']
feature_importance = rf_model.feature_importances_

# Get top 20 most important features
top_features_idx = np.argsort(feature_importance)[-20:]
top_features_names = [cols_mols[i] for i in top_features_idx]
top_features_importance = feature_importance[top_features_idx]

print("Top 20 Most Important Molecular Descriptors:")
for name, importance in zip(top_features_names, top_features_importance):
    print(f"{name}: {importance:.4f}")

# Plot feature importance
plt.figure(figsize=(12, 8))
plt.barh(range(len(top_features_names)), top_features_importance)
plt.yticks(range(len(top_features_names)), top_features_names)
plt.xlabel('Feature Importance')
plt.title('Top 20 Most Important Molecular Descriptors (Random Forest)')
plt.tight_layout()
plt.show()

# Model Comparison
print("\n=== Model Comparison ===")
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Test Accuracy': [results[name]['accuracy'] for name in results.keys()],
    'CV Accuracy (Mean)': [results[name]['cv_mean'] for name in results.keys()],
    'CV Accuracy (Std)': [results[name]['cv_std'] for name in results.keys()]
})

print(comparison_df.to_string(index=False))

# Plot model comparison
plt.figure(figsize=(10, 6))
x_pos = np.arange(len(results))
plt.bar(x_pos, [results[name]['accuracy'] for name in results.keys()], 
        yerr=[results[name]['cv_std'] for name in results.keys()], 
        capsize=5, alpha=0.7)
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Model Performance Comparison')
plt.xticks(x_pos, list(results.keys()), rotation=45)
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

# Confusion Matrix for best model
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_model = trained_models[best_model_name]
y_pred_best = results[best_model_name]['predictions']

plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=label_encoder.classes_, 
            yticklabels=label_encoder.classes_)
plt.title(f'Confusion Matrix - {best_model_name}')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# ROC Curve (if probabilities available)
if results[best_model_name]['probabilities'] is not None:
    plt.figure(figsize=(8, 6))
    fpr, tpr, _ = roc_curve(y_test, results[best_model_name]['probabilities'])
    auc_score = roc_auc_score(y_test, results[best_model_name]['probabilities'])
    
    plt.plot(fpr, tpr, label=f'{best_model_name} (AUC = {auc_score:.3f})')
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.tight_layout()
    plt.show()

# Save trained models
print("\n=== Saving Models ===")
os.makedirs('../results/models', exist_ok=True)

for name, model in trained_models.items():
    model_path = f'../results/models/{name.lower().replace(" ", "_")}_model.pkl'
    joblib.dump(model, model_path)
    print(f"Saved {name} model to {model_path}")

# Save label encoder
joblib.dump(label_encoder, '../results/models/label_encoder.pkl')
print("Saved label encoder")

# Save scaler
joblib.dump(scaler, '../results/models/feature_scaler.pkl')
print("Saved feature scaler")

# Save feature names
joblib.dump(cols_mols, '../results/models/feature_names.pkl')
print("Saved feature names")

print("\n=== Machine Learning Analysis Complete ===")
print(f"Best performing model: {best_model_name}")
print(f"Best accuracy: {results[best_model_name]['accuracy']:.3f}")

print("Script completed successfully!")
